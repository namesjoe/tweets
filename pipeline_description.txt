Описать свое видение решения, которое позволит выполнять ежедневно анализ согласно п.5. Из каких компонентов должно состоять решение, из каких шагов должен состоять ETL процесс от обработки входящих файлов до этапа сохранения конечной информации.
- на входе - непрерывный поток на FTP твитов в файлах (tweet.json), с частотой каждые три минуты. Размеры файлов - в среднем x10 от предоставленного сэмпла.
- на выходе - пользователи должны иметь возможность анализировать счастье по странам, локациям, пользователям, отслеживать изменения, собирать статиcтику и т.д.

1. каждые ХХ минут забираем все(!) доступные tweet_xxx.json. Проверяем что все файлы не пустые и вносим все имена в список переменной tweet_files_extracted
2. обрабатываем паралельно все файлы(airflow), подготавливая таблицы исходя из структуры бд
3. последовательно обновляем справочники users, countries 
4. данные для таблиц tweets tweet_sentiment сохраняем в txt на  S3 (к примеру) для оперативного bulk insert в бд
5. делаем bulk insert с S3
6. тесты на то что данные были добавлены
7. отправлем инстукцию на удаление файлов из ftp по сохраненному списку файлов tweet_files_extracted
8. если анализ согласно п.5 нужен ежедневно, то можно построить витрину куда вставлять результаты скрпитов пункта 5, добавив к ним поле current_date, чтобы было понятно когда было обновление и запустить по расписанию каждые 24ч

